# OINK ğŸ– â€” Overâ€‘provisioned Inference â€˜Nâ€™ Kit

A tiny **OpenAIâ€‘compatible** server that blatantly hogs your Mâ€‘series Macâ€™s resources in the name of *speed*. With a single Python file (`oink.py`) you get:

* **<â€¯1â€¯s firstâ€‘token latency** on an M3â€¯Ultra (with 4â€‘bit 7â€¯B models).
* The standard `/v1/chat/completions` (streaming + nonâ€‘stream) and `/v1/models` endpoints.
* Zero external services: MLX runs straight on Metal; FastAPI + Uvicorn serve HTTP.

> â€œWhy tiptoe when you can OINK?â€

---

## Table of Contents

1. [Features](#features)
2. [Prerequisites](#prerequisites)
3. [QuickÂ start](#quick-start)
4. [Configuration & tuning](#configuration--tuning)
5. [API examples](#api-examples)
6. [Benchmarks](#benchmarks)
7. [Troubleshooting](#troubleshooting)
8. [Roadmap](#roadmap)
9. [License](#license)

---

## Features

| âš¡ï¸ Feature                | Details                                                              |
| ------------------------- | -------------------------------------------------------------------- |
| **OpenAIÂ spec**           | `/v1/models` and `/v1/chat/completions` (*streaming* & *nonâ€‘stream*) |
| **Oneâ€‘file server**       | `oink.py` â€” 140Â ish lines, nothing else                              |
| **MLX backend**           | Loads any HF or local checkpoint that `mlx_lm` can convert/run       |
| **Aheadâ€‘ofâ€‘time compile** | `compile=True` + warmâ€‘up jit shrinks perâ€‘request latency             |
| **Quantization ready**    | 4â€‘bit QLoRA models run in <â€¯5Â GB VRAM                                |
| **RAMâ€‘lock helper**       | Script snippet bumps macOS â€œwiredâ€‘memoryâ€ limit so nothing swaps     |

---

## Prerequisites

* **macOSÂ 14.4+** (Sonoma) with an Appleâ€‘silicon GPU (M1/M2/M3).
  Tested on **MacÂ StudioÂ M3â€¯UltraÂ 512Â GB**.
* **PythonÂ 3.11+** â€” newer interpreters cut Metal shader compile time.
* **Xcode Commandâ€‘Line Tools** (`xcode-select --install`).

---

## QuickÂ start

```bash
# 1Â /Â Clone & activate venv
$ git clone https://github.com/DavidRagone/oink.git
$ cd oink && python3 -m venv venv
$ source venv/bin/activate && pip install -U pip

# 2Â /Â Install deps (FastAPI + Uvicorn + MLX)
$ pip install "fastapi>=0.111" "uvicorn[standard]>=0.29" "pydantic>=2.7" mlx_lm

# 3Â /Â Choose a model (HF repo or local path)
$ export MODEL_ID=mlx-community/Mistral-7B-Instruct-v0.3-4bit
# or, for example, use Qwen-1.5-7B:
$ export MODEL_ID=mlx-community/Qwen3-30B-A3B-8bit

# 4Â /Â Launch the pig ğŸš€
$ python oink.py --model $MODEL_ID --host 0.0.0.0 --port 8000
```

Expected log:

```text
[server] Loading model 'â€¦' â€¦ (first run can take ~10 s)
[server] Model ready â†’ starting API
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

---

## Configuration & tuning

| Option                 | CLI flag / env                                         | Effect                                                                |
| ---------------------- | ------------------------------------------------------ | --------------------------------------------------------------------- |
| **Model path / repo**  | `--model` or `MODEL_ID`                                | Any checkpoint `mlx_lm` understands (HF, local)                       |
| **Host / port**        | `--host`, `--port`                                     | Default `0.0.0.0:8000`                                                |
| **macOS wired memory** | `python oink.py --configure-macos`                     | Runs MLXâ€™s helper script to raise the pageâ€‘lock limit to 64Â GB        |
| **Workers**            | use `uvicorn` directly: `uvicorn oink:app --workers 4` | Each worker loads its own model copyâ€”512Â GB MacÂ Studio can handle 4â€“6 |

---

## API examples

### Curl (streaming)

```bash
curl -N http://localhost:8000/v1/chat/completions \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "'$MODEL_ID'",
    "stream": true,
    "messages": [{"role":"user","content":"Tell me a joke"}]
  }'
```

### OpenAI Python SDK

```python
import openai, os
openai.api_key  = os.getenv("OPENAI_API_KEY", "sk-local")
openai.base_url = "http://localhost:8000/v1"

chat = openai.ChatCompletion.create(
    model=os.environ["MODEL_ID"],
    messages=[{"role": "user", "content": "Ping?"}],
    stream=True,
)
for chunk in chat:
    print(chunk.choices[0].delta.content, end="", flush=True)
```

---

## Benchmarks

| Model (4â€‘bit)       | VRAM   | Tokens/s | 1stâ€‘token (70Â tok prompt) |
| ------------------- | ------ | -------- | ------------------------- |
| Mistralâ€‘7Bâ€‘Instruct | 3.9Â GB | 110â€“130  | \~0.35Â s                  |
| Qwenâ€‘1.5â€‘7B         | 4.3Â GB | 100â€“120  | \~0.38Â s                  |
| Phiâ€‘3â€‘miniâ€‘4.2B     | 2.1Â GB | 170+     | \~0.25Â s                  |

*Numbers from MacÂ Studio M3â€¯Ultra, macOSÂ 15.5, MLXÂ 0.26. Your mileage may vary.*

---

## Troubleshooting

| Symptom                               | Remedy                                                              |
| ------------------------------------- | ------------------------------------------------------------------- |
| `ModuleNotFoundError: mlx`            | Ensure `pip install mlx_lm` (not just `mlx`).                       |
| Metal â€œunsupported GPU familyâ€        | Update to macOSÂ 14.4+; M3 GPUs need it.                             |
| `OSError: address already in use`     | Change `--port` or kill the existing process (`lsof -i :8000`).     |
| High firstâ€‘token latency after reboot | First call recompiles Metal shaders; subsequent calls will be fast. |

---

## Roadmap / Niceâ€‘toâ€‘haves

* [ ] Embeddings endpoint (`/v1/embeddings`)
* [ ] Functionâ€‘calling / tool spec passthrough
* [ ] Simple LoRA hotâ€‘swap via `?adapter=â€¦` query
* [ ] Dockerfile (just for Intelâ€‘Linux bragging rights)

PRs welcomeâ€”OINK is deliberately simple but extensibility patches are gladly reviewed.

---

## License

MIT Â©Â 2025Â David Ragone.
Feel free to fork, extend, and, of course, **OINK** up those CPU cycles.
